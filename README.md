# Venture Data Validator
Enhancing data reliability: validating AI-extracted venture investment data with source text similarity.

## ðŸ’¡ Brief Explanation of the Used Approach
This project tackles the critical task of validating **pseudo-citations** generated by LLMs for structured venture investment data. Given that these citations can be prone to inaccuracies or "hallucinations," I developed an automated validation stage to streamline and optimize the human review process.

My approach centers on assessing the **likelihood of an extracted value being true** by meticulously comparing its associated pseudo-citation against the original source text. I achieve this through a sophisticated, multi-step pipeline:

### 1. ðŸ§¹ Text Preprocessing
Both the `source_website_text` and the `citation` strings undergo a rigorous preprocessing phase. This involves:

**Cleaning:** Removing structural elements like HTML tags, URLs, emails, and various forms of punctuation.
**Advanced Normalization:** Standardizing numerical expressions for dates, times, monetary values, quantities, ordinals, cardinals, and percentages.
**NER-Aware Handling:** Crucially, I apply **Named Entity Recognition (NER).** Identified entities (e.g., company names, locations) are handled with special care to preserve their meaning during subsequent steps like lemmatization and stop word removal. This prevents the unintentional alteration or deletion of vital information.

### 2. ðŸ§© Semantic Segmentation of Source Text
Instead of simply breaking the `source_website_text` into individual sentences, I intelligently divide it into **semantically coherent segments**. This is facilitated by a `SemanticSegmenter` that uses the `sentence-transformers/all-MiniLM-L6-v2` model. It generates embeddings for sentences and then groups them based on a cosine similarity threshold (e.g., 0.75). This ensures that related information, even if it spans multiple sentences, remains bundled together for a more accurate and contextual comparison.

### 3. âš¡ Efficient Similarity Search with FAISS
To enable rapid and efficient comparison of citations against potentially large source texts, I construct a **FAISS** (Facebook AI Similarity Search) index. This index is built from the embeddings of my semantically segmented source text. FAISS allows me to quickly and effectively retrieve **the most semantically similar segments** from the source text for any given citation.

### 4. âœ… Citation Validation
Finally, for each extracted citation:

An embedding is generated for the citation itself.
This citation embedding is then used to query the FAISS index, searching for segments within the preprocessed source text that exhibit high semantic similarity.
A citation is ultimately classified as **"likely true"** if its semantic similarity to at least one segment in the source text surpasses a predefined threshold (e.g., 0.62). Otherwise, it's flagged as **"likely untrue"** necessitating a mandatory manual review by a human.
This integrated approach combines robust text normalization with advanced semantic similarity techniques and efficient indexing to deliver a highly effective and scalable automated validation pipeline for venture investment data.

## Project structure:
```bash
Unicorn-Test-Task/
â”œâ”€â”€ main.py                    # Entry point for running citation verification
â”œâ”€â”€ text_preprocessor.py       # Preprocessing logic (data normalization)
â”œâ”€â”€ semantic_segmenter.py      # Segmentation of source text using semantic similarity
â”œâ”€â”€ similarity_search.py       # FAISS-based similarity search utilities
â”œâ”€â”€ data.json                  # Input data with source texts and citations
â”œâ”€â”€ data_updated.json          # Output file with factuality results
â”œâ”€â”€ requirements.txt           # All Python package dependencies
â””â”€â”€ README.md                  # Documentation and usage instructions
```
## Instructions for Running Locally
**1. Clone the repository:**
```bash
git clone https://github.com/maijamana/VentureDataValidator.git
cd VentureDataValidator
```
**2. Install dependencies:**
```bash
pip install -r requirements.txt
```
**3. Download the required SpaCy model:**
```bash
python -m spacy download en_core_web_sm
```
**5. Run the main script:**
```bash
python main.py
```
This will process the `data.json` file and write the results with factuality estimates to `data_updated.json`.

## ðŸ“ˆ Analysis of the Algorithm's Business Performance
This algorithm significantly boosts the reliability of venture investment data extracted by LLMs, directly impacting the efficiency of manual reviews and overall data quality.

### Threshold-Based Reliability
The core of this algorithm's performance is its similarity threshold, which classifies data as "likely true" or "likely untrue." This threshold directly manages the trade-off between precision (avoiding false positives) and recall (identifying all true positives).

**For High Data Criticality (threshold â‰ˆ0.7):** A higher threshold prioritizes accuracy, reducing false positives. This means fewer incorrect "likely true" classifications, though it might send more truly correct data for manual review. Choose this if data veracity is paramount.

**For Error Tolerance (threshold 0.5âˆ’0.6):** A lower threshold captures more "likely true" values, potentially reducing manual effort. However, this may allow more false positives to pass through automated validation. Opt for this if your business process can accommodate some errors to minimize workload.
Future Enhancements for Business Value

### Considerations for Future Enhancements

While effective, the algorithm can be further optimized for the specific nuances of venture investment data:

**1. Specialized Entity Handling:** Implement dedicated parsers for domain-specific entities like locations and monetary values (e.g., correctly interpreting "$10M" as "10 million"). This ensures greater accuracy in validation.

**2. Link Preservation:** Consider preserving URLs and links during preprocessing. If citations refer to specific links in the source text, retaining this information could improve validation accuracy by maintaining contextual relevance.

By strategically adjusting the threshold and implementing these enhancements, the algorithm can be continuously refined to meet evolving business demands for data accuracy and operational efficiency.
